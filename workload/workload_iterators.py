# Workloadè¿­ä»£å™¨ - å¯¹åº”Workload.ccä¸­çš„å„ç§è¿­ä»£æ–¹æ³•
# å¯¹åº”C++å‡½æ•°ç­¾åï¼š
# - void Workload::iterate_micro_benchmark()
# - void Workload::iterate_data_parallel()
# - void Workload::iterate_hybrid_parallel_Transformer()
# - void Workload::iterate_hybrid_parallel_Transformer_fwd_in_bckwd()
# - void Workload::iterate_hybrid_parallel_DLRM()
# - void Workload::iterate_model_parallel()
# - void Workload::iterate_hybrid_parallel_data_model()
# - void Workload::iterate_hybrid_parallel_model_data()
# - void Workload::iterate_distributed_inference()
# - void Workload::iterate_hybrid_parallel_customized()

from typing import List, Dict, Any, Optional
from system.common import EventType, ComType, SchedulingPolicy, CollectiveBarrier
from .parallelism_policy import LoopState
import rich

class WorkloadIterators:
    """å·¥ä½œè´Ÿè½½è¿­ä»£å™¨ - è´Ÿè´£å„ç§å¹¶è¡Œç­–ç•¥çš„è¿­ä»£é€»è¾‘"""
    
    def __init__(self, workload):
        """
        åˆå§‹åŒ–è¿­ä»£å™¨
        
        Args:
            workload: å·¥ä½œè´Ÿè½½å¯¹è±¡
        """
        self.workload = workload
    
    def iterate_micro_benchmark(self):
        """
        å¾®åŸºå‡†æµ‹è¯•è¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_micro_benchmark()
        """
        assert 0 <= self.workload.index < self.workload.size
        if self.workload.current_state != LoopState.Wait_For_Sim_Finish:
            for pass_counter in range(self.workload.total_pass):
                self.workload.layers[self.workload.index].issue_weight_grad_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Non_Blocking)
            # æ›´æ–°æˆå‘˜å˜é‡ä»¥åæ˜ æ‰€æœ‰ pass éƒ½å·²å®Œæˆ
            self.workload.pass_counter = self.workload.total_pass
        self.workload.check_for_sim_end()
    def iterate_data_parallel(self):
        """
        æ•°æ®å¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_data_parallel()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Weight_Gradient
                self.workload.index -= 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            self.workload.delay_loaded = False
            self.workload.layers[self.workload.index].issue_weight_grad_comm(
                SchedulingPolicy.None_, CollectiveBarrier.Non_Blocking)
            
            if self.workload.index == 0:
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            self.workload.delay_loaded = False
            self.workload.index -= 1
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_hybrid_parallel_transformer(self):
        """
        Transformeræ··åˆå¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_hybrid_parallel_Transformer()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Input_Gradient
                self.workload.index -= 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_weight_grad_comm(
                    SchedulingPolicy.FIFO, CollectiveBarrier.Non_Blocking)
            
            if not self.workload.layers[self.workload.index].is_input_grad_comm_finished_blocking():
                return
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            if self.workload.index >= 0:
                self.workload.index -= 1
            if self.workload.index == -1:
                self.workload.index = 0
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_input_grad_comm(
                    SchedulingPolicy.LIFO, CollectiveBarrier.Blocking)
                return
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_hybrid_parallel_dlrm(self):
        """
        DLRMæ··åˆå¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_hybrid_parallel_DLRM()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued and self.workload.layers[self.workload.index].fwd_pass_comm_type == ComType.All_to_All:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.HIGHEST, CollectiveBarrier.Non_Blocking)
            elif self.workload.index == self.workload.dlrm_last_bottom_layer:
                if not self.workload.layers[0].is_fwd_pass_comm_finished_blocking():
                    return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Weight_Gradient
                self.workload.index -= 1
            
            if self.workload.generator.id == 0:
                print(f"layer changed to: {self.workload.index}")
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_weight_grad_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Non_Blocking)
            
            if (self.workload.parallelism_policy == ParallelismPolicy.DLRM and
                not self.workload.layers[self.workload.index].is_input_grad_comm_finished_blocking()):
                return
            
            if self.workload.index == 0:
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if self.workload.index == self.workload.dlrm_last_bottom_layer + 1:
                self.workload.layers[0].issue_input_grad_comm(
                    SchedulingPolicy.HIGHEST, CollectiveBarrier.Non_Blocking)
            
            self.workload.index -= 1
            if self.workload.generator.id == 0:
                print(f"layer changed to: {self.workload.index} in ig")
            
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
    
    def iterate_model_parallel(self):
        """
        æ¨¡å‹å¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_model_parallel()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                involved_dimensions = [True, True, True]
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Input_Gradient
                self.workload.index -= 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.layers[self.workload.index].is_input_grad_comm_finished_blocking():
                return
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            if self.workload.index >= 0:
                self.workload.index -= 1
            if self.workload.index == -1:
                self.workload.index = 0
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued and self.workload.index > 0:
                self.workload.collective_issued = True
                involved_dimensions = [True, True, True]
                self.workload.layers[self.workload.index].issue_input_grad_comm(
                    SchedulingPolicy.LIFO, CollectiveBarrier.Non_Blocking)
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_hybrid_parallel_data_model(self):
        """
        æ•°æ®æ¨¡å‹æ··åˆå¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_hybrid_parallel_data_model()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Input_Gradient
                self.workload.index -= 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_weight_grad_comm(
                    SchedulingPolicy.FIFO, CollectiveBarrier.Non_Blocking)
            
            if not self.workload.layers[self.workload.index].is_input_grad_comm_finished_blocking():
                return
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            if self.workload.index >= 0:
                self.workload.index -= 1
            if self.workload.index == -1:
                self.workload.index = 0
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued and self.workload.index > 0:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_input_grad_comm(
                    SchedulingPolicy.LIFO, CollectiveBarrier.Non_Blocking)
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_hybrid_parallel_model_data(self):
        """
        æ¨¡å‹æ•°æ®æ··åˆå¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_hybrid_parallel_model_data()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Input_Gradient
                self.workload.index -= 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_weight_grad_comm(
                    SchedulingPolicy.FIFO, CollectiveBarrier.Non_Blocking)
            
            if not self.workload.layers[self.workload.index].is_input_grad_comm_finished_blocking():
                return
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            if self.workload.index >= 0:
                self.workload.index -= 1
            if self.workload.index == -1:
                self.workload.index = 0
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued and self.workload.index > 0:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_input_grad_comm(
                    SchedulingPolicy.LIFO, CollectiveBarrier.Non_Blocking)
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_distributed_inference(self):
        """
        åˆ†å¸ƒå¼æ¨ç†è¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_distributed_inference()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.index = 0
                self.workload.pass_counter += 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_hybrid_parallel_transformer_fwd_in_bckwd(self):
        """
        Transformerå‰å‘åå‘æ··åˆå¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_hybrid_parallel_Transformer_fwd_in_bckwd()
        """
        # è·å–MockNcclLogå®ä¾‹ - å¯¹åº”C++ç‰ˆæœ¬
        from system.mock_nccl_log import MockNcclLog, NcclLogLevel
        NcclLog = MockNcclLog.getInstance()
        
        # æ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼Œå¯¹åº”C++ç‰ˆæœ¬çš„æ–­è¨€
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        NcclLog.writeLog(NcclLogLevel.INFO, f'Transformerå‰å‘åå‘æ··åˆå¹¶è¡Œè¿­ä»£')
        NcclLog.writeLog(NcclLogLevel.INFO, f"è¿›å…¥WorkloadIteratorsè¿­ä»£å™¨ - å½“å‰çŠ¶æ€: {self.workload.current_state}, å±‚ç´¢å¼•: {self.workload.index}, æ€»å±‚æ•°: {self.workload.size}")
        
        if self.workload.current_state == LoopState.Forward_Pass:
            NcclLog.writeLog(NcclLogLevel.INFO, f"=== å¤„ç†å‰å‘ä¼ æ’­é˜¶æ®µ - å±‚ {self.workload.index}/{self.workload.size-1} ===")
            
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} æƒé‡æ¢¯åº¦é€šä¿¡æœªå®Œæˆï¼Œç­‰å¾…ä¸­...")
                return
                
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} è®¾ç½®å‰å‘è®¡ç®—æ—¶é—´: {self.workload.counter}")
            
            if self.workload.counter > 0:
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} æ³¨å†Œå‰å‘è®¡ç®—ç­‰å¾…äº‹ä»¶ï¼Œç­‰å¾…æ—¶é—´: {self.workload.counter}")
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                # è°ƒæ•´é€šä¿¡å¤§å°ï¼ˆå¦‚æœå°äº4096ä¸”å¤§äº0ï¼‰
                if (self.workload.layers[self.workload.index].fwd_pass_comm_size < 4096 and 
                    self.workload.layers[self.workload.index].fwd_pass_comm_size > 0):
                    old_size = self.workload.layers[self.workload.index].fwd_pass_comm_size
                    self.workload.layers[self.workload.index].fwd_pass_comm_size = 4096
                    NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} è°ƒæ•´å‰å‘é€šä¿¡å¤§å°: {old_size} -> 4096")
                
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} å‘èµ·å‰å‘ä¼ æ’­é€šä¿¡ï¼Œå¤§å°: {self.workload.layers[self.workload.index].fwd_pass_comm_size}")
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            # é€šä¿¡å·²å‘èµ·ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ­¥ï¼ˆå¯¹åº”C++ç‰ˆæœ¬çš„ç¬¬äºŒæ¬¡è°ƒç”¨ï¼‰
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                NcclLog.writeLog(NcclLogLevel.INFO, f"ğŸ‰ å‰å‘ä¼ æ’­é˜¶æ®µå®Œæˆï¼æ‰€æœ‰ {self.workload.size} å±‚å·²å¤„ç†ï¼Œåˆ‡æ¢åˆ°è¾“å…¥æ¢¯åº¦é˜¶æ®µ")
                self.workload.current_state = LoopState.Input_Gradient
                self.workload.index -= 1
            else:
                NcclLog.writeLog(NcclLogLevel.INFO, f"â­ï¸ å‰å‘ä¼ æ’­ç»§ç»­ï¼Œç§»åŠ¨åˆ°ç¬¬ {self.workload.index} å±‚ï¼ˆå…± {self.workload.size} å±‚ï¼‰")
            
            # æ·»åŠ æ—¥å¿—è®°å½• - å¯¹åº”C++ç‰ˆæœ¬
            NcclLog.writeLog(NcclLogLevel.DEBUG, "workload::call fwd_pass register_event EventType::General ")
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            NcclLog.writeLog(NcclLogLevel.INFO, f"=== å¤„ç†æƒé‡æ¢¯åº¦é˜¶æ®µ - å±‚ {self.workload.index}/{self.workload.size-1} ===")
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} è®¾ç½®æƒé‡æ¢¯åº¦è®¡ç®—æ—¶é—´: {self.workload.counter}")
            
            if self.workload.counter > 0:
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} æ³¨å†Œæƒé‡æ¢¯åº¦è®¡ç®—ç­‰å¾…äº‹ä»¶ï¼Œç­‰å¾…æ—¶é—´: {self.workload.counter}")
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            self.workload.delay_loaded = False
            NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} å‘èµ·æƒé‡æ¢¯åº¦é€šä¿¡")
            self.workload.layers[self.workload.index].issue_weight_grad_comm(
                SchedulingPolicy.None_, CollectiveBarrier.Non_Blocking)
            
            # å…³é”®ä¿®å¤ï¼šåœ¨æƒé‡æ¢¯åº¦é˜¶æ®µéœ€è¦é€’å‡index
            if self.workload.index == 0:
                NcclLog.writeLog(NcclLogLevel.INFO, f"ğŸ æƒé‡æ¢¯åº¦é˜¶æ®µå®Œæˆï¼ç¬¬ {self.workload.pass_counter} è½®å®Œæˆäºæ—¶é—´: {self.workload.generator.boostedTick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
                self.workload.index = 0  # é‡ç½®indexä¸º0å¼€å§‹ä¸‹ä¸€è½®
                NcclLog.writeLog(NcclLogLevel.INFO, f"å¼€å§‹ç¬¬ {self.workload.pass_counter} è½®ï¼Œè¿”å›å‰å‘ä¼ æ’­é˜¶æ®µ")
                # æ£€æŸ¥æ¨¡æ‹Ÿæ˜¯å¦åº”è¯¥ç»“æŸ
                self.workload.check_for_sim_end()
                if self.workload.current_state == LoopState.Wait_For_Sim_Finish:
                    NcclLog.writeLog(NcclLogLevel.INFO, f"ğŸŠ æ‰€æœ‰è½®æ¬¡å®Œæˆï¼æ¨¡æ‹Ÿç»“æŸ")
                    return
            else:
                # æƒé‡æ¢¯åº¦é˜¶æ®µå¤„ç†åï¼Œé€’å‡index
                self.workload.index -= 1
                NcclLog.writeLog(NcclLogLevel.INFO, f"æƒé‡æ¢¯åº¦é€’å‡åˆ°å±‚ {self.workload.index}ï¼Œç»§ç»­æƒé‡æ¢¯åº¦é˜¶æ®µ")
                # ä¿æŒåœ¨æƒé‡æ¢¯åº¦é˜¶æ®µç»§ç»­å¤„ç†ä¸‹ä¸€å±‚
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            NcclLog.writeLog(NcclLogLevel.INFO, f"=== å¤„ç†è¾“å…¥æ¢¯åº¦é˜¶æ®µ - å±‚ {self.workload.index}/{self.workload.size-1} ===")
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} è®¾ç½®è¾“å…¥æ¢¯åº¦è®¡ç®—æ—¶é—´: {self.workload.counter}")
            
            if self.workload.counter > 0:
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} æ³¨å†Œè¾“å…¥æ¢¯åº¦è®¡ç®—ç­‰å¾…äº‹ä»¶ï¼Œç­‰å¾…æ—¶é—´: {self.workload.counter}")
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.layers[self.workload.index].is_fwd_pass_comm_finished_blocking():
                NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} å‰å‘é€šä¿¡æœªå®Œæˆï¼Œç­‰å¾…ä¸­...")
                return
            
            self.workload.delay_loaded = False
            NcclLog.writeLog(NcclLogLevel.INFO, f"å±‚ {self.workload.index} å‘èµ·è¾“å…¥æ¢¯åº¦é€šä¿¡")
            self.workload.layers[self.workload.index].issue_input_grad_comm(
                SchedulingPolicy.None_, CollectiveBarrier.Non_Blocking)
            
            if self.workload.index >= 0:
                self.workload.index -= 1
            
            if self.workload.index == -1:
                NcclLog.writeLog(NcclLogLevel.INFO, f"ğŸ“‰ è¾“å…¥æ¢¯åº¦é˜¶æ®µå®Œæˆï¼åˆ‡æ¢åˆ°æƒé‡æ¢¯åº¦é˜¶æ®µ")
                self.workload.index = self.workload.size - 1
                self.workload.current_state = LoopState.Weight_Gradient
            else:
                NcclLog.writeLog(NcclLogLevel.INFO, f"â¬…ï¸ è¾“å…¥æ¢¯åº¦ç»§ç»­ï¼Œç§»åŠ¨åˆ°ç¬¬ {self.workload.index} å±‚")
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Forward_In_BackPass:
            NcclLog.writeLog(NcclLogLevel.INFO, f"å¤„ç†åå‘ä¼ æ’­ä¸­çš„å‰å‘é˜¶æ®µ - å±‚ {self.workload.index}")
            
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                NcclLog.writeLog(NcclLogLevel.INFO, f"ç­‰å¾…å±‚ {self.workload.index} çš„æƒé‡æ¢¯åº¦é€šä¿¡å®Œæˆï¼ˆåå‘å‰å‘é˜¶æ®µï¼‰")
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
                NcclLog.writeLog(NcclLogLevel.INFO, f"è®¾ç½®å±‚ {self.workload.index} åå‘å‰å‘è®¡ç®—æ—¶é—´: {self.workload.counter}")
            
            if self.workload.counter > 0:
                NcclLog.writeLog(NcclLogLevel.INFO, f"æ³¨å†Œå±‚ {self.workload.index} åå‘å‰å‘è®¡ç®—ç­‰å¾…äº‹ä»¶ï¼Œç­‰å¾…æ—¶é—´: {self.workload.counter}")
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                NcclLog.writeLog(NcclLogLevel.INFO, f"å‘èµ·å±‚ {self.workload.index} åå‘å‰å‘é€šä¿¡")
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            # æ·»åŠ è¾¹ç•Œæ£€æŸ¥ - è¿™æ˜¯å…³é”®ä¿®å¤
            if self.workload.index < self.workload.size and self.workload.layers[self.workload.index].needs_fwd_in_bckwd_initiation:
                self.workload.current_state = LoopState.Input_Gradient
                NcclLog.writeLog(NcclLogLevel.INFO, f"åå‘å‰å‘é˜¶æ®µå®Œæˆï¼Œè¿”å›è¾“å…¥æ¢¯åº¦é˜¶æ®µï¼Œå±‚: {self.workload.index}")
            else:
                NcclLog.writeLog(NcclLogLevel.INFO, f"åå‘å‰å‘é˜¶æ®µç»§ç»­ï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€å±‚: {self.workload.index}")
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
    
    def iterate_hybrid_parallel_customized(self):
        """
        è‡ªå®šä¹‰æ··åˆå¹¶è¡Œè¿­ä»£ - å¯¹åº”C++å‡½æ•°
        void Workload::iterate_hybrid_parallel_customized()
        """
        assert 0 <= self.workload.index < self.workload.size
        self.workload.check_for_sim_end()
        
        if self.workload.current_state == LoopState.Forward_Pass:
            if not self.workload.layers[self.workload.index].is_weight_grad_comm_finished_blocking():
                return
            
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_fwd_pass_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_forward_pass_comm(
                    SchedulingPolicy.None_, CollectiveBarrier.Blocking)
                return
            
            self.workload.index += 1
            self.workload.delay_loaded = False
            self.workload.collective_issued = False
            if self.workload.index >= self.workload.size:
                self.workload.current_state = LoopState.Input_Gradient
                self.workload.index -= 1
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Weight_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_weight_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_weight_grad_comm(
                    SchedulingPolicy.FIFO, CollectiveBarrier.Non_Blocking)
            
            if not self.workload.layers[self.workload.index].is_input_grad_comm_finished_blocking():
                return
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            if self.workload.index >= 0:
                self.workload.index -= 1
            if self.workload.index == -1:
                self.workload.index = 0
                if self.workload.generator.id == 0:
                    print(f"pass: {self.workload.pass_counter} finished at time: {self.workload.generator.get_tick()}")
                self.workload.pass_counter += 1
                self.workload.current_state = LoopState.Forward_Pass
            else:
                self.workload.current_state = LoopState.Input_Gradient
            
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return
            
        elif self.workload.current_state == LoopState.Input_Gradient:
            if not self.workload.delay_loaded:
                self.workload.counter = self.workload.layers[self.workload.index].get_input_grad_compute()
                self.workload.delay_loaded = True
            
            if self.workload.counter > 0:
                self.workload.generator.try_register_event(
                    self.workload, EventType.Workload_Wait, None, self.workload.counter)
                return
            
            if not self.workload.collective_issued and self.workload.index > 0:
                self.workload.collective_issued = True
                self.workload.layers[self.workload.index].issue_input_grad_comm(
                    SchedulingPolicy.LIFO, CollectiveBarrier.Non_Blocking)
            
            self.workload.collective_issued = False
            self.workload.delay_loaded = False
            self.workload.current_state = LoopState.Weight_Gradient
            self.workload.generator.register_event(self.workload, EventType.General, None, 1)
            return 